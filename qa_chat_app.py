import streamlit as st
from typing_extensions import TypedDict
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate
from langchain_core.messages import BaseMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from typing import List
from dotenv import load_dotenv
load_dotenv()

import os
import torch

torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)] 

# or simply:
torch.classes.__path__ = []

llm = ChatOpenAI(model="gpt-4o-mini")

model_name = 'BAAI/bge-m3'
embeddings = HuggingFaceEmbeddings(model_name=model_name)

vector_store = Chroma(
    collection_name="qa_chat_collection",
    embedding_function=embeddings,
    persist_directory=".chroma_db_bge-m3/", 
)

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents
        chat_history: list of messages
    """

    question: str
    generation: str
    documents: List[str]
    chat_history: List[BaseMessage]

def init_page():
    st.set_page_config(
        page_title="AI Chat Demo App",
        page_icon="ğŸ¤–"
    )
    st.header("AI Chat Demo Application ğŸ¤–")
    st.sidebar.title("Options")

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = []

def retrieve(state):
    """
    Retrieve documents
    Args:
        state (dict): The current graph state
    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents  
    """
    print("---RETRIEVE---")
    question = state["question"]
    retriever = vector_store.as_retriever(
        search_type="mmr", search_kwargs={"k": 3, "fetch_k": 5}
    )
    documents = retriever.invoke(question)
    print(f"retrieve documents: {documents}")
    return {"documents": documents, "question": question}

def generate(state):
    
    print("---GENERATE---")
    documents = state["documents"]
    docs_contents =  "\n\n".join([doc.page_content for doc in documents])

    system_message_prompt = SystemMessagePromptTemplate.from_template("""
ã‚ãªãŸã¯è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ¤œç´¢ã•ã‚ŒãŸä»¥ä¸‹ã®æ–‡è„ˆã¨ä¼šè©±å±¥æ­´ã®ä¸€éƒ¨ã‚’ä½¿ã£ã¦è³ªå•ã«ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ç­”ãˆãŒã‚ã‹ã‚‰ãªã‘ã‚Œã°ã€ã‚ã‹ã‚‰ãªã„ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
æœ€å¤§ã§3ã¤ã®æ–‡ç« ã‚’ä½¿ã„ã€ç°¡æ½”ãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
                                                                  
æ–‡è„ˆ:
====
{context}
====
""")

    prompt = ChatPromptTemplate.from_messages([
        system_message_prompt,
        MessagesPlaceholder("messages"),
    ])
    messages = prompt.invoke(
        {
            'context': docs_contents,
            'messages': state["chat_history"]
        }
    )
    print(f"chat prompt: {messages}")
    response = llm.invoke(messages)
    return {'generation': response}

def main():
    init_page()

    workflow = StateGraph(state_schema=GraphState)

    workflow.add_node("retrieve", retrieve)
    workflow.add_node("generate", generate)
    workflow.add_edge(START, "retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)

    memory = MemorySaver()
    graph = workflow.compile(checkpointer=memory)

    # thread_idã¯ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ä¿å­˜ã•ã‚ŒãŸå„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ä¸€æ„ã® ID ã§ã™ã€‚
    config = {"configurable": {"thread_id": "abc123"}}
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    init_messages()
    
    # ã‚¢ãƒ—ãƒªã®å†å®Ÿè¡Œã®éš›ã«å±¥æ­´ã®ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
    if user_input := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": user_input})
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.chat_message("human").write(user_input)
        with st.spinner("QABot is typing ..."):
            response = graph.invoke(
                {
                    "question": user_input,
                    "chat_history": st.session_state.messages
                 },
                config=config,
            )
            answer = response['generation'].content
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "assistant", "content": answer}) 
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.chat_message("ai").write(answer)


if __name__ == '__main__':
    main()